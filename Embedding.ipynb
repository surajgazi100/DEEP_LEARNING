{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled59.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7CaObQ3XV14tJjDX9Qv2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajgazi100/DEEP_LEARNING/blob/main/Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS3QLv7xbLmi"
      },
      "source": [
        "# How to Use Word Embedding Layers for Deep Learning with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WsJ6NegbPb-"
      },
      "source": [
        "Word Embedding\n",
        "\n",
        "Keras Embedding Layer\n",
        "\n",
        "Example of Learning an Embedding\n",
        "\n",
        "Example of Using Pre-Trained GloVe Embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sX9fj2abac2"
      },
      "source": [
        "put data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
        "\n",
        "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
        "\n",
        "It is a flexible layer that can be used in a variety of ways, such as:\n",
        "\n",
        "It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
        "It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
        "It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
        "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
        "\n",
        "It must specify 3 arguments:\n",
        "\n",
        "input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
        "output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
        "input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
        "For example, below we define an Embedding layer with a vocabulary of 200 (e.g. integer encoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be embedded, and input documents that have 50 words each.\n",
        "\n",
        "e = Embedding(200, 32, input_length=50)\n",
        "1\n",
        "e = Embedding(200, 32, input_length=50)\n",
        "The Embedding layer has weights that are learned. If you save your model to file, this will include weights for the Embedding layer.\n",
        "\n",
        "The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).\n",
        "\n",
        "If you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.\n",
        "\n",
        "Now, let’s see how we can use an Embedding layer in practice.\n",
        "\n",
        "3. Example of Learning an Embedding\n",
        "In this section, we will look at how we can learn a word embedding while fitting a neural network on a text classification problem.\n",
        "\n",
        "We will define a small problem where we have 10 text documents, each with a comment about a piece of work a student submitted. Each text document is classified as positive “1” or negative “0”. This is a simple sentiment analysis problem.\n",
        "\n",
        "First, we will define the documents and their class labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akoGuOBycYeg",
        "outputId": "491ed399-f56f-4397-ae76-feb313563d26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install embeddings"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting embeddings\n",
            "  Downloading https://files.pythonhosted.org/packages/bd/da/55d07bcdaac48b293aa88d797be3d89f6b960e2f71565dd64204fa0b6a4f/embeddings-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from embeddings) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from embeddings) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from embeddings) (1.18.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->embeddings) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->embeddings) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->embeddings) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->embeddings) (2.10)\n",
            "Installing collected packages: embeddings\n",
            "Successfully installed embeddings-0.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZPJNdJ8bfhO",
        "outputId": "00399b24-f27a-47b0-9e82-2fc5a12cc9cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[30, 11], [39, 13], [16, 8], [15, 13], [12], [37], [23, 8], [46, 39], [23, 13], [5, 20, 11, 27]]\n",
            "[[30 11  0  0]\n",
            " [39 13  0  0]\n",
            " [16  8  0  0]\n",
            " [15 13  0  0]\n",
            " [12  0  0  0]\n",
            " [37  0  0  0]\n",
            " [23  8  0  0]\n",
            " [46 39  0  0]\n",
            " [23 13  0  0]\n",
            " [ 5 20 11 27]]\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy: 80.000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hFc7IuZbIHt",
        "outputId": "7e97a3f5-13da-4c6f-cf56-e2ef6fa0bf3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!',\n",
        "\t\t'Weak',\n",
        "\t\t'Poor effort!',\n",
        "\t\t'not good',\n",
        "\t\t'poor work',\n",
        "\t\t'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "print(model.summary())\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[30, 11], [39, 13], [16, 8], [15, 13], [12], [37], [23, 8], [46, 39], [23, 13], [5, 20, 11, 27]]\n",
            "[[30 11  0  0]\n",
            " [39 13  0  0]\n",
            " [16  8  0  0]\n",
            " [15 13  0  0]\n",
            " [12  0  0  0]\n",
            " [37  0  0  0]\n",
            " [23  8  0  0]\n",
            " [46 39  0  0]\n",
            " [23 13  0  0]\n",
            " [ 5 20 11 27]]\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 4, 8)              400       \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Accuracy: 80.000001\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}